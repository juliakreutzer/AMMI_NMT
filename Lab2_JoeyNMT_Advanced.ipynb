{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_JoeyNMT_Advanced.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliakreutzer/AMMI_NMT/blob/main/Lab2_JoeyNMT_Advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRwdfvuQ486V"
      },
      "source": [
        "# Lab 2: Joey NMT Advanced\n",
        "\n",
        "In this notebook, we'll train a Transformer model for translating between TED talks in French (*fr*) and English (*en*). We'll do some configuration debugging and experiment with hyperparameters, then inspect evaluation metrics and find out how robust the model is.\n",
        "\n",
        "The pre-processing code is a bit lengthy, but it reflects reality: often getting the data into the right format and selecting the right pieces takes more code than the actual model training ;) \n",
        "\n",
        "At the very end of this colab you'll also find instructions on how to get started with backtranslation as a data augmentation technique, and how to build a multilingual model. These topics are not mandatory but might be fun to explore if you have time. \n",
        "\n",
        "**Important:** Before you start, set runtime type to GPU.\n",
        "\n",
        "Author: Julia Kreutzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx1neLQt3vZ0"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "0t24FKds4_oj",
        "outputId": "5ff78b36-38cf-4ab2-b8dd-9c25b345c555"
      },
      "source": [
        "!pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5MB)\n",
            "\u001b[K     |████████████████████████████████| 763.5MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.20.1)\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.8.0\n",
            "    Uninstalling torch-1.8.0:\n",
            "      Successfully uninstalled torch-1.8.0\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g_-Qu5xo5ILb",
        "outputId": "18dcd2af-8768-4d1d-cd82-3d992301c492"
      },
      "source": [
        "!pip install joeynmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting joeynmt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/0a/f383560ca9eedbd4a09f5d9d1523aaf3b4b5e0e79a1e5d9c72ff370826d4/joeynmt-1.3-py3-none-any.whl (84kB)\n",
            "\r\u001b[K     |███▉                            | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 20kB 28.2MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 30kB 21.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 40kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 51kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 61kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 71kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 81kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 8.0MB/s \n",
            "\u001b[?25hCollecting torch==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/99/5861239a6e1ffe66e120f114a4d67e96e5c4b17c1a785dfc6ca6769585fc/torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5MB 24kB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt) (7.1.2)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (2.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (56.1.0)\n",
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt) (3.2.2)\n",
            "Collecting numpy==1.20.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8a/064b4077e3d793f877e3b77aa64f56fa49a4d37236a53f78ee28be009a16/numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 208kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.16.0)\n",
            "Collecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/f0/9705d6ec002876bc20b6923cbdeeca82569a895fc214211562580e946079/pylint-2.8.2-py3-none-any.whl (357kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 57.4MB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/50/84184d6230686e230c464f0dd4ff32eada2756b4a0b9cefec68b88d1d580/torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 19.1MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 49.1MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (2.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.8.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.8.1)\n",
            "Collecting astroid<2.7,>=2.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/82/a61df6c2d68f3ae3ad1afa0d2e5ba5cfb7386eb80cffb453def7c5757271/astroid-2.5.6-py3-none-any.whl (219kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 54.2MB/s \n",
            "\u001b[?25hCollecting isort<6,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/47/0ec3ec948b7b3a0ba44e62adede4dca8b5985ba6aaee59998bed0916bd17/isort-5.8.0-py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 56.1MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (4.41.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (1.24.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.2.8)\n",
            "Collecting lazy-object-proxy>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/b0/f055db25fd68ab4859832a887c8b304274fc12dd5a3f8e83e61250733aeb/lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt) (2018.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.4.8)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68385 sha256=1947fa83a29e94887bbad49d365e5ed9693034d7b920ba2fde2039c59b7ff053\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, torch, six, subword-nmt, wrapt, lazy-object-proxy, typed-ast, astroid, isort, mccabe, pylint, torchtext, pyyaml, portalocker, sacrebleu, joeynmt\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.5.6 isort-5.8.0 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.8.2 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torch-1.8.0 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNHhMgH55P8l"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "We'll use *English - French* translations from the [IWSLT 2017 challenge](https://wit3.fbk.eu/2017-01-c), the [\"unofficial\" task](https://sites.google.com/site/iwsltevaluation2017/TED-tasks?authuser=0). This challenge is about translating TED talks from multiple languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KrEuHrz5dEt"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTCwLsM5_He"
      },
      "source": [
        "Requires downloading a file of 292MB. If you do this ahead of time, store a copy of it in your Google drive and access it from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qUEf7DX5whA",
        "outputId": "7b3e0cae-ef5f-4f3a-bd4f-31231a677a9f"
      },
      "source": [
        "! pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3B9ejpL5LFx",
        "outputId": "996fc3eb-e734-4b07-beae-a713953a383e"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1gFeuPTRc3RB4DhJEkhr8O-a8PObM7Ix2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gFeuPTRc3RB4DhJEkhr8O-a8PObM7Ix2\n",
            "To: /content/2017-01-trnted.tgz\n",
            "292MB [00:01, 198MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJuXNbBA5ly6",
        "outputId": "047b8f7e-937e-4415-a290-711022ab5d2f"
      },
      "source": [
        "! tar -zxvf /content/2017-01-trnted.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2017-01-trnted/\n",
            "2017-01-trnted/texts/\n",
            "2017-01-trnted/._texts.html\n",
            "2017-01-trnted/texts.html\n",
            "2017-01-trnted/texts/ar/\n",
            "2017-01-trnted/texts/de/\n",
            "2017-01-trnted/texts/en/\n",
            "2017-01-trnted/texts/fr/\n",
            "2017-01-trnted/texts/ja/\n",
            "2017-01-trnted/texts/ko/\n",
            "2017-01-trnted/texts/zh/\n",
            "2017-01-trnted/texts/zh/en/\n",
            "2017-01-trnted/texts/zh/en/._.eval\n",
            "2017-01-trnted/texts/zh/en/.eval\n",
            "2017-01-trnted/texts/zh/en/._.info\n",
            "2017-01-trnted/texts/zh/en/.info\n",
            "2017-01-trnted/texts/zh/en/._zh-en.tgz\n",
            "2017-01-trnted/texts/zh/en/zh-en.tgz\n",
            "2017-01-trnted/texts/ko/en/\n",
            "2017-01-trnted/texts/ko/en/._.eval\n",
            "2017-01-trnted/texts/ko/en/.eval\n",
            "2017-01-trnted/texts/ko/en/._.info\n",
            "2017-01-trnted/texts/ko/en/.info\n",
            "2017-01-trnted/texts/ko/en/._ko-en.tgz\n",
            "2017-01-trnted/texts/ko/en/ko-en.tgz\n",
            "2017-01-trnted/texts/ja/en/\n",
            "2017-01-trnted/texts/ja/en/._.eval\n",
            "2017-01-trnted/texts/ja/en/.eval\n",
            "2017-01-trnted/texts/ja/en/._.info\n",
            "2017-01-trnted/texts/ja/en/.info\n",
            "2017-01-trnted/texts/ja/en/._ja-en.tgz\n",
            "2017-01-trnted/texts/ja/en/ja-en.tgz\n",
            "2017-01-trnted/texts/fr/en/\n",
            "2017-01-trnted/texts/fr/en/._.eval\n",
            "2017-01-trnted/texts/fr/en/.eval\n",
            "2017-01-trnted/texts/fr/en/._.info\n",
            "2017-01-trnted/texts/fr/en/.info\n",
            "2017-01-trnted/texts/fr/en/._fr-en.tgz\n",
            "2017-01-trnted/texts/fr/en/fr-en.tgz\n",
            "2017-01-trnted/texts/en/ar/\n",
            "2017-01-trnted/texts/en/de/\n",
            "2017-01-trnted/texts/en/fr/\n",
            "2017-01-trnted/texts/en/ja/\n",
            "2017-01-trnted/texts/en/ko/\n",
            "2017-01-trnted/texts/en/zh/\n",
            "2017-01-trnted/texts/en/zh/._.eval\n",
            "2017-01-trnted/texts/en/zh/.eval\n",
            "2017-01-trnted/texts/en/zh/._.info\n",
            "2017-01-trnted/texts/en/zh/.info\n",
            "2017-01-trnted/texts/en/zh/._en-zh.tgz\n",
            "2017-01-trnted/texts/en/zh/en-zh.tgz\n",
            "2017-01-trnted/texts/en/ko/._.eval\n",
            "2017-01-trnted/texts/en/ko/.eval\n",
            "2017-01-trnted/texts/en/ko/._.info\n",
            "2017-01-trnted/texts/en/ko/.info\n",
            "2017-01-trnted/texts/en/ko/._en-ko.tgz\n",
            "2017-01-trnted/texts/en/ko/en-ko.tgz\n",
            "2017-01-trnted/texts/en/ja/._.eval\n",
            "2017-01-trnted/texts/en/ja/.eval\n",
            "2017-01-trnted/texts/en/ja/._.info\n",
            "2017-01-trnted/texts/en/ja/.info\n",
            "2017-01-trnted/texts/en/ja/._en-ja.tgz\n",
            "2017-01-trnted/texts/en/ja/en-ja.tgz\n",
            "2017-01-trnted/texts/en/fr/._.eval\n",
            "2017-01-trnted/texts/en/fr/.eval\n",
            "2017-01-trnted/texts/en/fr/._.info\n",
            "2017-01-trnted/texts/en/fr/.info\n",
            "2017-01-trnted/texts/en/fr/._en-fr.tgz\n",
            "2017-01-trnted/texts/en/fr/en-fr.tgz\n",
            "2017-01-trnted/texts/en/de/._.eval\n",
            "2017-01-trnted/texts/en/de/.eval\n",
            "2017-01-trnted/texts/en/de/._.info\n",
            "2017-01-trnted/texts/en/de/.info\n",
            "2017-01-trnted/texts/en/de/._en-de.tgz\n",
            "2017-01-trnted/texts/en/de/en-de.tgz\n",
            "2017-01-trnted/texts/en/ar/._.eval\n",
            "2017-01-trnted/texts/en/ar/.eval\n",
            "2017-01-trnted/texts/en/ar/._.info\n",
            "2017-01-trnted/texts/en/ar/.info\n",
            "2017-01-trnted/texts/en/ar/._en-ar.tgz\n",
            "2017-01-trnted/texts/en/ar/en-ar.tgz\n",
            "2017-01-trnted/texts/de/en/\n",
            "2017-01-trnted/texts/de/en/._.eval\n",
            "2017-01-trnted/texts/de/en/.eval\n",
            "2017-01-trnted/texts/de/en/._.info\n",
            "2017-01-trnted/texts/de/en/.info\n",
            "2017-01-trnted/texts/de/en/._de-en.tgz\n",
            "2017-01-trnted/texts/de/en/de-en.tgz\n",
            "2017-01-trnted/texts/ar/en/\n",
            "2017-01-trnted/texts/ar/en/._.eval\n",
            "2017-01-trnted/texts/ar/en/.eval\n",
            "2017-01-trnted/texts/ar/en/._.info\n",
            "2017-01-trnted/texts/ar/en/.info\n",
            "2017-01-trnted/texts/ar/en/._ar-en.tgz\n",
            "2017-01-trnted/texts/ar/en/ar-en.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHrN51fJ6cjJ"
      },
      "source": [
        "The `texts` subdirectory contains translation data for multiple languages. Let's start with `fr-en`, French to English translations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAVpplSI6YW-",
        "outputId": "52c0aa2c-d7b4-441f-b287-eaac98bb68a2"
      },
      "source": [
        "!tar -xvf 2017-01-trnted/texts/fr/en/fr-en.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fr-en/\n",
            "fr-en/IWSLT17.TED.dev2010.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.dev2010.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2010.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2010.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2011.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2011.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2012.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2012.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2013.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2013.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2014.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2014.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2015.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2015.fr-en.fr.xml\n",
            "fr-en/README\n",
            "fr-en/train.en\n",
            "fr-en/train.tags.fr-en.en\n",
            "fr-en/train.tags.fr-en.fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pNHfnZHnzb"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGjjucDo7ZtO"
      },
      "source": [
        "The parallel data is stored in XML, see the description in the README. But it's multiple documents per file, so XML parsing requires splitting it. We'll go the quick and dirty way, as in this [pre-processing script](https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh) by just removing all metainformation that we're not interested in (i.e. every line containing a html tag. This is *not a good example for mindful pre-processing*, but good enough for now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjktf_ky6qgi",
        "outputId": "121ce4ff-b204-4827-bb71-6581e86c50ec"
      },
      "source": [
        "! head -n 20 /content/fr-en/train.tags.fr-en.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<doc docid=\"1\" genre=\"lectures\"> \n",
            "<url>http://www.ted.com/talks/al_gore_on_averting_climate_crisis</url> \n",
            "<keywords>talks, alternative energy, cars, climate change, culture, environment, global issues, politics, science, sustainability, technology</keywords> \n",
            "<speaker>Al Gore</speaker> \n",
            "<talkid>1</talkid> \n",
            "<title>Al Gore: Averting the climate crisis</title> \n",
            "<description>TED Talk Subtitles and Transcript: With the same humor and humanity he exuded in \"An Inconvenient Truth,\" Al Gore spells out 15 ways that individuals can address climate change immediately, from buying a hybrid to inventing a new, hotter brand name for global warming.</description> \n",
            "Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. \n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. \n",
            "And I say that sincerely, partly because  Put yourselves in my position. \n",
            "I flew on Air Force Two for eight years. \n",
            "Now I have to take off my shoes or boots to get on an airplane! \n",
            " I'll tell you one quick story to illustrate what that's been like for me. \n",
            "It's a true story -- every bit of this is true. \n",
            "Soon after Tipper and I left the --  White House --  we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville. \n",
            "Driving ourselves. \n",
            "I know it sounds like a little thing to you, but -- I looked in the rear-view mirror and all of a sudden it just hit me. \n",
            "There was no motorcade back there. \n",
            "You've heard of phantom limb pain?  This was a rented Ford Taurus.  It was dinnertime, and we started looking for a place to eat. \n",
            "We were on I-40. We got to Exit 238, Lebanon, Tennessee. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhXOSyKVBRae"
      },
      "source": [
        "def remove_xml(filename):\n",
        "  \"\"\"Remove all lines that contain xml brackets except for those in <seg>.\"\"\"\n",
        "  valid_lines = []\n",
        "  with open(filename, 'r') as ofile:\n",
        "    for line in ofile:\n",
        "      if ('<' in line or '>' in line) and not '<seg' in line:\n",
        "        continue\n",
        "      else:\n",
        "        # Get content between <seg> tags for dev and test sets.\n",
        "        if '<seg' in line:\n",
        "          content = line.strip().split('>')[1].split('<')[0]\n",
        "        else: \n",
        "          content = line.strip()\n",
        "        valid_lines.append(content)\n",
        "  return valid_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N5gyBpz8jeW",
        "outputId": "9a245038-9da8-454a-fa5d-7969e65c8eda"
      },
      "source": [
        "targets = remove_xml('/content/fr-en/train.tags.fr-en.en')\n",
        "print(f'Read {len(targets)} target sentences.')\n",
        "    \n",
        "sources = remove_xml('/content/fr-en/train.tags.fr-en.fr')\n",
        "print(f'Read {len(sources)} source sentences.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 232825 target sentences.\n",
            "Read 232825 source sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNoL2WyF_rgD"
      },
      "source": [
        "Let's check if they match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFgKyecW8nSP",
        "outputId": "fb40cdf2-e633-40a6-9052-af086ead2ddd"
      },
      "source": [
        "num_examples = 3\n",
        "for s, t in zip(sources[:num_examples], targets[:num_examples]):\n",
        "  print(s)\n",
        "  print(t)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\n",
            "Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "\n",
            "J'ai été très impressionné par cette conférence, et je tiens à vous remercier tous pour vos nombreux et sympathiques commentaires sur ce que j'ai dit l'autre soir.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "\n",
            "Et je dis çà sincèrement, en autres parce que --Faux sanglot-- j'en ai besoin ! --Rires-- Mettez-vous à ma place!\n",
            "And I say that sincerely, partly because  Put yourselves in my position.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iais2WO6AIa5"
      },
      "source": [
        "Looks good! You might already see that the translations are sometimes not very literal. Let's write them into file to feed them to Joey NMT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DwIq9Su_2Pl"
      },
      "source": [
        "def write_to_file(sentences, filename):\n",
        "  \"\"\"Write sentences to file.\"\"\"\n",
        "  with open(filename, 'w') as ofile:\n",
        "    for sent in sentences:\n",
        "      ofile.write(sent.strip()+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B7ZLZo2KyiS"
      },
      "source": [
        "data_dir = '/content/fr-en'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ-aWXlGHCvv"
      },
      "source": [
        "file_prefix = 'parallel_'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL0rTleLHI5k"
      },
      "source": [
        "src_lang = 'fr'\n",
        "trg_lang = 'en'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve2BE5mcBPuO"
      },
      "source": [
        "train_src_file = os.path.join(data_dir, file_prefix+'train.'+src_lang)\n",
        "train_trg_file = os.path.join(data_dir, file_prefix+'train.'+trg_lang)\n",
        "write_to_file(targets, train_trg_file)\n",
        "write_to_file(sources, train_src_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IckyC5M9Cv3-"
      },
      "source": [
        "Great, now we need a development and a test set. As development set we can pick any of the `tst` or `dev` files in the data we just downloaded (these were used for testing and evaluation for previous years). We'll go with `tst2015` for testing and `tst2014` for development."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR2g1vWgFQLy"
      },
      "source": [
        "**Question for you**: Is this choice important? How do you think selecting a different dev/test set could influence our findings?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO_zFPcbFOHT",
        "outputId": "d91eb7ee-5854-4259-9e0f-823d3802e8c3"
      },
      "source": [
        "test_targets = remove_xml('/content/fr-en/IWSLT17.TED.tst2015.fr-en.en.xml')\n",
        "print(f'Read {len(test_targets)} test target sentences.')\n",
        "    \n",
        "test_sources = remove_xml('/content/fr-en/IWSLT17.TED.tst2015.fr-en.fr.xml')\n",
        "print(f'Read {len(test_sources)} test source sentences.')\n",
        "\n",
        "dev_targets = remove_xml('/content/fr-en/IWSLT17.TED.tst2014.fr-en.en.xml')\n",
        "print(f'Read {len(dev_targets)} dev target sentences.')\n",
        "    \n",
        "dev_sources = remove_xml('/content/fr-en/IWSLT17.TED.tst2014.fr-en.fr.xml')\n",
        "print(f'Read {len(dev_sources)} dev source sentences.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 1210 test target sentences.\n",
            "Read 1210 test source sentences.\n",
            "Read 1306 dev target sentences.\n",
            "Read 1306 dev source sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_UAV5oiFpmu"
      },
      "source": [
        "dev_src_file = os.path.join(data_dir, file_prefix+'dev.'+src_lang)\n",
        "dev_trg_file = os.path.join(data_dir, file_prefix+'dev.'+trg_lang)\n",
        "test_src_file = os.path.join(data_dir, file_prefix+'test.'+src_lang)\n",
        "test_trg_file = os.path.join(data_dir, file_prefix+'test.'+trg_lang)\n",
        "\n",
        "write_to_file(dev_targets, dev_trg_file)\n",
        "write_to_file(dev_sources, dev_src_file)\n",
        "write_to_file(test_targets, test_trg_file)\n",
        "write_to_file(test_sources, test_src_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOClab0XHhvh"
      },
      "source": [
        "## Sub-words\n",
        "\n",
        "Same procedure as in Lab 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R_1HRBvHBn-"
      },
      "source": [
        "bpe_size = 4000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDxhoKCuKnPE"
      },
      "source": [
        "train_joint_file = os.path.join(data_dir, file_prefix+'train.'+src_lang+'-'+trg_lang)\n",
        "\n",
        "src_files = {'train': train_src_file, 'dev': dev_src_file, 'test': test_src_file}\n",
        "trg_files = {'train': train_trg_file, 'dev': dev_trg_file, 'test': test_trg_file}\n",
        "\n",
        "vocab_src_file = os.path.join(data_dir, f'vocab.{bpe_size}.{src_lang}')\n",
        "vocab_trg_file = os.path.join(data_dir, f'vocab.{bpe_size}.{trg_lang}')\n",
        "bpe_file = os.path.join(data_dir, f'bpe.codes.{bpe_size}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9OpCuhsKpIZ"
      },
      "source": [
        "! cat $train_src_file $train_trg_file > $train_joint_file\n",
        "\n",
        "! subword-nmt learn-bpe \\\n",
        "  --input $train_joint_file \\\n",
        "  -s $bpe_size \\\n",
        "  -o $bpe_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZiHb1iVKsWS"
      },
      "source": [
        "src_bpe_files = {}\n",
        "trg_bpe_files = {}\n",
        "for split in ['train', 'dev', 'test']:\n",
        "  src_input_file = src_files[split]\n",
        "  trg_input_file = trg_files[split]\n",
        "  src_output_file = src_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n",
        "  trg_output_file = trg_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n",
        "  src_bpe_files[split] = src_output_file\n",
        "  trg_bpe_files[split] = trg_output_file\n",
        "\n",
        "  ! subword-nmt apply-bpe \\\n",
        "    -c $bpe_file \\\n",
        "    < $src_input_file > $src_output_file\n",
        "\n",
        "  ! subword-nmt apply-bpe \\\n",
        "    -c $bpe_file \\\n",
        "    < $trg_input_file > $trg_output_file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3TMw58tKu5M",
        "outputId": "ed393585-b0d9-442d-d12d-475e7993094b"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-16 21:33:10--  https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2034 (2.0K) [text/plain]\n",
            "Saving to: ‘build_vocab.py’\n",
            "\n",
            "\rbuild_vocab.py        0%[                    ]       0  --.-KB/s               \rbuild_vocab.py      100%[===================>]   1.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-16 21:33:10 (22.1 MB/s) - ‘build_vocab.py’ saved [2034/2034]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IzclnElKwkg"
      },
      "source": [
        "vocab_src_file = src_bpe_files['train']\n",
        "vocab_trg_file = trg_bpe_files['train']\n",
        "bpe_vocab_file = os.path.join(data_dir, f'joint.{bpe_size}bpe.vocab')\n",
        "\n",
        "! python build_vocab.py  \\\n",
        "  $vocab_src_file $vocab_trg_file \\\n",
        "  --output_path $bpe_vocab_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U92TAIbLO0vV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOswhM_cTkVu",
        "outputId": "0d9ae81e-924f-4ea8-d93a-25f51ea7add2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive_home = '/content/drive'\n",
        "drive.mount(drive_home)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoiHXv_STskk"
      },
      "source": [
        "g_drive_path = \"/content/drive/My\\ Drive/NMT_Lab2/models/%s-%s\" % (src_lang, trg_lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfX4_9cGTXHL"
      },
      "source": [
        "experiment_name = 'ted_fr_en'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJh0MywhTzhq"
      },
      "source": [
        "model_path = os.path.join(g_drive_path, experiment_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLm8TR2LFbVX"
      },
      "source": [
        "Copy the BPE merges to Gdrive so we don't lose them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoLrEKP9FY5e"
      },
      "source": [
        "bpe_drive_path = \"/content/drive/My\\ Drive/NMT_Lab2/bpe/%s-%s\" % (src_lang, trg_lang)\n",
        "! mkdir -p $bpe_drive_path\n",
        "! cp $bpe_file $bpe_drive_path "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvYBPK2nO2us"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "The following configuration file contains *three* bugs that prevent it from working (=quickly giving reasonable BLEU for translating between French and English). Find and fix those three. Try not to compare with the config from Lab 1 ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG6ftq_2PVIu"
      },
      "source": [
        "# Create the config\n",
        "broken_config = \"\"\"\n",
        "name: \"{name}\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{source_language}\"\n",
        "    train: \"{data_dir}/parallel_train.{bpe_size}\"  \n",
        "    dev:   \"{data_dir}/parallel_dev.{bpe_size}.bpe\"\n",
        "    test:  \"{data_dir}/parallel_test.{bpe_size}.bpe\"\n",
        "    level: \"bpe\"                   # Here we specify we're working on BPEs.\n",
        "    lowercase: False                \n",
        "    max_sent_length: 30             # Extend to longer sentences.\n",
        "    src_vocab: \"{src_vocab_path}\"\n",
        "    trg_vocab: \"{trg_vocab_path}\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "    sacrebleu:                      # sacrebleu options\n",
        "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
        "        tokenize: \"intl\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
        "\n",
        "training:\n",
        "    #load_model: \"{model_path}/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # Alternative: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 1.0\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 500          # Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"{model_path}\"\n",
        "    overwrite: False                 # Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True        # Joint vocabulary.\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # Increase to 512 for larger data.\n",
        "        ff_size: 1024            # Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=experiment_name, \n",
        "           source_language=src_lang, \n",
        "           target_language=trg_lang,\n",
        "           data_dir=data_dir, \n",
        "           model_path=model_path, \n",
        "           src_vocab_path=bpe_vocab_file,\n",
        "           trg_vocab_path=bpe_vocab_file, \n",
        "           bpe_size=bpe_size)\n",
        "with open(\"transformer_{name}.yaml\".format(name=experiment_name),'w') as f:\n",
        "    f.write(broken_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF14SqV0UbfM"
      },
      "source": [
        "If you try running this training multiple times for debugging, set `overwrite` to `True` in the config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNILzUckUFFv",
        "outputId": "0b98f41c-15e0-409f-a82d-ea25f127440d"
      },
      "source": [
        "!python -m joeynmt train transformer_ted_fr_en.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/training.py\", line 766, in train\n",
            "    \"overwrite\", False))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/helpers.py\", line 43, in make_model_dir\n",
            "    \"Model directory exists and overwriting is disabled.\")\n",
            "FileExistsError: Model directory exists and overwriting is disabled.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3D6LbFGWV_7"
      },
      "source": [
        "If correct, the model should obtain roughly the following BLEU scores (or better!):\n",
        "\n",
        "\n",
        "*   Step 500: 0.05\n",
        "*   Step 1000: 1.24\n",
        "*   Step 1500: 1.97\n",
        "*   Step 2000: 3.71\n",
        "*   ...\n",
        "*   Step 3000: 7.12\n",
        "*   Step 8000: 15.30\n",
        "*   Step 17000: 21.69\n",
        "*   Step 27000: 23.67  (around 2h training time)\n",
        "\n",
        "You don't need to wait that long for the purpose of this exercise, Julia can provide a checkpoint for an already trained model :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8DIatjWHxQk"
      },
      "source": [
        "**TODO:**\n",
        "1. Why does the number of source words reported in the log not match the specified number of BPE merges? Tip: browse the [subword-nmt GitHub](https://github.com/rsennrich/subword-nmt).\n",
        "2. Imagine your job is to provide the best translation system as soon as possible. Try changing a few hyperparameters to see what the best score is that you can get within three epochs of training. You may also coordinate this with your colleagues.\n",
        "  * Suggestions: try changing bpe size, learning rate, batch size. \n",
        "  * Recommendation: create a new configuration and experiment directory for each experiment so you can tell them apart.\n",
        "  * You can spend endless time on this, but try to select a few settings that you'd hope could improve the result. \n",
        "  * Do you observe any tendency? Compare with your colleagues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxFq5xUbBZrt"
      },
      "source": [
        "*Notes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3d3xwjVCJ3n"
      },
      "source": [
        "\n",
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQpIPDeKYpFA"
      },
      "source": [
        "For the following exercises you may either use your own model or the trained one provided by Julia (trained for 30 epochs).\n",
        "\n",
        "Now that we got a trained model, let's see how well it does. We'll probe for the following examples:\n",
        "\n",
        "1. A *training/memorization/overfitting* check: Did model learn to perfectly translate the training set?\n",
        "2. Unseen but from the *same domain*: Did the model learn to generalize to unseen examples?\n",
        "3. *Out-of-domain*: Can the model translate a random sentence from the source language?\n",
        "\n",
        "It will be increasingly hard for the model to do well on these. But even in the training set you can probably find outliers that the model does not translate well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJDr8Fw3EONl"
      },
      "source": [
        "from joeynmt.helpers import load_config\n",
        "import yaml\n",
        "\n",
        "\n",
        "def download_pretrained_model_from_gdrive(\n",
        "    checkpoint='1sIaNogftpt-moKEKRBMAKbE4QdOb7XwM',\n",
        "    config='1_FpHfRn8bxLu_pAUgj99jtZuBcCABgXV',\n",
        "    src_vocabulary='1esULLiG-2fS6Ucj2LMndUoID8WaY_QuZ',\n",
        "    trg_vocabulary='1sdygCZxK6h8M1khDlh-TLAq0Y4DNczl_',\n",
        "    bpe_merges='17XeygY048oXQHHzmH4u_hiJl1GndkiSN',\n",
        "    directory='/content/pretrained_model'):\n",
        "  \n",
        "  \"\"\"Download pretrained model from ids and place it in given directory. \n",
        "  Adjust paths in config as needed.\n",
        "  Default ids are for a model as specified above, but trained for the full\n",
        "  30 epochs. \n",
        "  \"\"\"\n",
        "\n",
        "  # Download files and place them into the new directory.\n",
        "  original_config = os.path.join(directory, 'original_config.yaml')\n",
        "  new_config = os.path.join(directory, 'config.yaml')\n",
        "  checkpoint_path = os.path.join(directory, 'best.ckpt')\n",
        "  trg_vocab_path = os.path.join(directory, 'trg_vocab.txt')\n",
        "  src_vocab_path = os.path.join(directory, 'src_vocab.txt')\n",
        "  bpe_path = os.path.join(directory, 'bpe.merges')\n",
        "\n",
        "  def gdown_by_id(id, output):\n",
        "    ! gdown 'https://drive.google.com/uc?id='$id -O $output\n",
        "\n",
        "  ! mkdir -p $directory\n",
        "  gdown_by_id(checkpoint, checkpoint_path)\n",
        "  gdown_by_id(config, original_config)\n",
        "  gdown_by_id(src_vocabulary, src_vocab_path)\n",
        "  gdown_by_id(trg_vocabulary, trg_vocab_path)\n",
        "  gdown_by_id(bpe_merges, bpe_path)\n",
        "\n",
        "  # Overwrite paths in config.\n",
        "  config = load_config(original_config)\n",
        "  config['data']['src_vocab'] = src_vocab_path\n",
        "  config['data']['trg_vocab'] = trg_vocab_path\n",
        "  config['training']['model_dir'] = directory\n",
        "  config['training']['load_model'] = checkpoint_path\n",
        "  with open(new_config, 'w') as cfile:\n",
        "    yaml.dump(config, cfile)\n",
        "  return new_config, bpe_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYaF8k1cDA9G",
        "outputId": "61003b7d-18b2-410c-9111-d82908f83970"
      },
      "source": [
        "# Download a pretrained model.\n",
        "pretrained_config, pretrained_bpe = download_pretrained_model_from_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sIaNogftpt-moKEKRBMAKbE4QdOb7XwM\n",
            "To: /content/pretrained_model/best.ckpt\n",
            "157MB [00:00, 192MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_FpHfRn8bxLu_pAUgj99jtZuBcCABgXV\n",
            "To: /content/pretrained_model/original_config.yaml\n",
            "100% 3.70k/3.70k [00:00<00:00, 6.13MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1esULLiG-2fS6Ucj2LMndUoID8WaY_QuZ\n",
            "To: /content/pretrained_model/src_vocab.txt\n",
            "100% 26.3k/26.3k [00:00<00:00, 22.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sdygCZxK6h8M1khDlh-TLAq0Y4DNczl_\n",
            "To: /content/pretrained_model/trg_vocab.txt\n",
            "100% 26.3k/26.3k [00:00<00:00, 6.05MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17XeygY048oXQHHzmH4u_hiJl1GndkiSN\n",
            "To: /content/pretrained_model/bpe.merges\n",
            "100% 33.8k/33.8k [00:00<00:00, 29.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYRhVqjMH4RU"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "\n",
        "1.   Pick 2-5 sentences each from the three sets described above and translate them with your model in `translate` mode. Remember that you need to split them into BPEs first (already done for 1 and 2; example code for that in Lab 1).\n",
        "2.   Compare their translations: Can you tell from these examples what kind of data the model was trained on? Anything surprisingly good or bad?\n",
        "3. Choose one sentence that the model translated really well. Can you perturb it so that it's still very similar to the original but the translation is very different or significantly worse? \n",
        "\n",
        "Small changes in the input leading to small changes in the output can be seen as a criterion for robustness. The harder it is to find these adversarial inputs, the more robust is the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rIihpclD5tm",
        "outputId": "cb2c5556-b870-40ac-80bf-7658a416a4d2"
      },
      "source": [
        "# Either use $pretrained_config for the pretrained model or your own trained model.\n",
        "!python -m joeynmt translate $pretrained_config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-16 21:59:08,488 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-05-16 21:59:16,267 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-05-16 21:59:16,475 - INFO - joeynmt.model - Enc-dec model built.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "Don@@ e.\n",
            "JoeyNMT: Hypotheses ranked by score\n",
            "JoeyNMT #1: Done.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "This is the L@@ iv@@ ing@@ st@@ on Pu@@ bli@@ c Li@@ br@@ ary that was comple@@ ted in 200@@ 4 in my hom@@ et@@ ow@@ n, and, you know, it's got a dom@@ e and it's got this rou@@ nd thing and col@@ um@@ n@@ s, red br@@ ic@@ k, and you can kind of gu@@ ess what L@@ iv@@ ing@@ st@@ on is trying to say with this build@@ ing@@ : childr@@ en, pro@@ per@@ ty val@@ ues and histor@@ y.\n",
            "JoeyNMT: Hypotheses ranked by score\n",
            "JoeyNMT #1: This is from Livingston Audience: Library is completed in 2004 or my hometown, he was born.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "Cela nous ren@@ d donc un peu cré@@ du@@ les et très très pré@@ visi@@ bl@@ es.\n",
            "JoeyNMT: Hypotheses ranked by score\n",
            "JoeyNMT #1: So this makes us a little credules and very predictable.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "\n",
            "Bye.\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2B83tnyDopt"
      },
      "source": [
        "*Notes:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1yq7rq0I1UF"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG1UC2rfKewE"
      },
      "source": [
        "We now got an intuition of what the model can do and where its limits are. During validation, we trust the BLEU score to tell us whether the model is progressing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdPc2LjGK3-S"
      },
      "source": [
        "1.   Compute the `sacrebleu` ([GitHub](https://github.com/mjpost/sacrebleu)) score for the dev set translations for a chosen step that are stored in your model directory (`.hyps`). Below is an example call. Do they match with the result that was reported in `validations.txt` and `train.log`?\n",
        "2.   In the configuration we chose one particular tokenizer, but there are other options (hint: explore sacrebleu documentation). Does the reported score change? If so, why do you think this happens?\n",
        "3. The `sacrebleu` library also implements the ChrF score. Compute the ChrF as well as the BLEU score for two validation steps. How do differ with respect to ChrF and BLEU, are the differences comparable?\n",
        "\n",
        "(We did not tokenize our data before feeding it to the model. Do you think it makes a difference? You can try it out with the `sacremoses` library that implements tokenizers.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHkNRNIrUHVP"
      },
      "source": [
        "# Helper function\n",
        "def read_sentences(inputfile):\n",
        "  \"\"\"Read sentences from file into list.\"\"\"\n",
        "  lines = []\n",
        "  with open(inputfile, 'r') as ofile:\n",
        "    for line in ofile:\n",
        "      lines.append(line.strip())\n",
        "  print(f'Read {len(lines)} sentences from file {inputfile}.')\n",
        "  return lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e13-fIRnUogy"
      },
      "source": [
        "# Model outputs\n",
        "hyps = read_sentences('path/to/.hyps')\n",
        "# And references for the same dev set\n",
        "refs = read_sentences('path/to/references')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9q9lYJnRwFY"
      },
      "source": [
        "sacrebleu.corpus_bleu(hyps, [refs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk_0TAeRU-29"
      },
      "source": [
        "Note the one-element list that we're passing to the BLEU score calculation. This is because BLEU was originally proposed to compute quality scores relative to multiple translations. However, in practice there are rarely multiple translations available, so we got to work with what we have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCzlKnCZKs1j"
      },
      "source": [
        "# Extra: Backtranslation & Multilingual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VhSYBwNRcVM"
      },
      "source": [
        "These experiments take more time than you'll have in the lab and relate to contents covered later this week. They might be interesting to explore if you want to keep learning about NMT :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX9_V4veK7fM"
      },
      "source": [
        "### Backtranslation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRq7DZ4FYhgE"
      },
      "source": [
        "The downloaded data also contains a `train.en` file: monolingual data for English. This can be used to improve our model with backtranslation.  There are multiple steps and options involved:\n",
        "  * First, you need to train a en-fr model.\n",
        "  * Then use this reverse model to translate this monolingual data (or a part thereof, depending on translation speed).\n",
        "  * Now you have synthetic training data that you can either 1) mix with the original training data as it is, 2) mix with a certain ratio, since this data has probably lower quality. \n",
        "  * You can then either 1) further train the original `fr-en` model on this data, or 2) retrain a `fr-en` model to see if it gets better than the original data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McyAT7BPYidz"
      },
      "source": [
        "*Notes:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q7Do7l1K_JF"
      },
      "source": [
        "### Multilingual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jsgyXYDLCuG"
      },
      "source": [
        "The downloaded directory also contains for other languages paired with English on the target side: `ar`, `de`, `ja`, `ko`, `zh`. Additional training data from other languages often helps to improve translation quality for small training data. \n",
        "\n",
        "We'll try out the \"many-to-one\" approach here: learning to translate from many languages into English. For the opposite, we would need to add special target language tags to the source (.e.g. `<2fr>`, `<2ja>` to tell the model which language it should translate into.\n",
        "\n",
        "* First, select one or more language pairs to add to fr-en.\n",
        "* Repeat the pre-processing pipeline for them. Training and dev sets should get concatenated for joint training. BPE training should also be done on a concatenation of the training sets for all languages, so that the sub-word merges reflect all languages.\n",
        "* Depending on the number of languages, your concatenated dev set might grow too large for regular validation during training, so you can also just take a smaller subset from each language and combine them. \n",
        "* Do you find improvements over the original model? For a direct comparison you would need to translate the original fr-en dev or test set with the multilingual model (not the concatenated ones used for this experiment) and compare the scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deylviR3RZGI"
      },
      "source": [
        "*Notes:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UstIWIwKRaVp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}